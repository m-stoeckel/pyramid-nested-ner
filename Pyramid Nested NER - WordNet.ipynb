{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFx0z_eJmowS"
   },
   "source": [
    "# Pyramid: A Layered Model for Nested Named Entity Recognition - GENIA Experiment\n",
    "\n",
    "This notebook tries to reproduce the experiments on the GENIA dataset in [Pyramid: A Layered Model for Nested Named Entity Recognition](https://www.aclweb.org/anthology/2020.acl-main.525.pdf). Keep in mind that the split that I was able to recover on the web, is not the same used by the authors (the original dataset is not free), but the distribution is very close. The notebook includes a brief data analysis section to quantitavely assess this difference.\n",
    "\n",
    "**Turn on the GPUs!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/m-stoeckel/pyramid-nested-ner.git\n",
    "!mv pyramid-nested-ner/* . && rm -rf pyramid-nested-ner # move to root"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install flair seqeval"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gzip\n",
    "import requests\n",
    "import os.path\n",
    "\n",
    "!mkdir data\n",
    "!mkdir genia/glove/\n",
    "\n",
    "for url, file in [\n",
    "    ('https://h2880387.stratoserver.net/seafile/f/8df93eaee16348049d38/?dl=1', 'data/wordnet-dataset.dev.jsonl'),\n",
    "    ('https://h2880387.stratoserver.net/seafile/f/b459ef1196c64d6f9bf5/?dl=1', 'data/wordnet-dataset.test.jsonl'),\n",
    "    ('https://h2880387.stratoserver.net/seafile/f/e22f95803c9b4bcdb71f/?dl=1', 'data/wordnet-dataset.train.jsonl'),\n",
    "    ('https://h2880387.stratoserver.net/seafile/f/ebbb5f39446843b89796/?dl=1', 'genia/glove/glove.6B.200d.txt')\n",
    "]:\n",
    "  if os.path.isfile(file):\n",
    "    continue\n",
    "  with open(file, 'wb') as f:\n",
    "    print(f\"Downloading '{file}'\")\n",
    "    resp = requests.get(url, verify=False)\n",
    "    print(f\"Decompressing '{file}'\")\n",
    "    f.write(gzip.decompress(resp.content))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.stats import norm\n",
    "\n",
    "from genia.utils.load_wv import get_bio_word_vectors\n",
    "from pyramid_nested_ner.data.dataset import PyramidNerDataset\n",
    "from pyramid_nested_ner.model import PyramidNer\n",
    "from pyramid_nested_ner.modules.word_embeddings.pretrained_embeddings import PretrainedWordEmbeddings\n",
    "from pyramid_nested_ner.training.optim import get_default_sgd_optim\n",
    "from pyramid_nested_ner.training.trainer import PyramidNerTrainer\n",
    "from pyramid_nested_ner.utils.data import jsonline_data_reader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5iF3VdA1kI2N"
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgSCoL8Znder"
   },
   "outputs": [],
   "source": [
    "def load_wordnet_split(split='train'):\n",
    "    with open(f\"data/wordnet-dataset.{split}.jsonl\", 'r') as fp:\n",
    "        return [json.loads(line) for line in fp]\n",
    "\n",
    "train, test, dev = load_wordnet_split('train'), load_wordnet_split('test'), load_wordnet_split('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5-XH5Z7a4NW"
   },
   "outputs": [],
   "source": [
    "def plot_histogram(df, bins=20, title=None):\n",
    "  fig, axes = plt.subplots(len(df.index), 2, figsize=(16, 96))\n",
    "  if title:\n",
    "    fig.suptitle(title, fontsize=24, y=1.01)\n",
    "  for i, col in enumerate(df.columns):\n",
    "    for j, (_, row) in enumerate(df.iterrows()):\n",
    "      view = np.array(row[col])\n",
    "      axes[j, i].set_title(f\"{df.index.values[j]} {col}\")\n",
    "      axes[j, i].hist(view, bins=bins, density=True)\n",
    "      x = np.arange(view.min(), view.max() + 1)\n",
    "      axes[j, i].plot(\n",
    "        x, norm.pdf(x, view.mean(), view.std())\n",
    "      )\n",
    "  fig.tight_layout(pad=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iVYhC16tDKG6"
   },
   "outputs": [],
   "source": [
    "def piecharts(*pies, titles=None, fig_title=None):\n",
    "  titles = titles or [\" - \" for pie in pies]\n",
    "  fig, axes = plt.subplots(1, len(pies), figsize=(21, 8))\n",
    "\n",
    "  def autopct_func(pct, allvals):\n",
    "    absolute = round(pct/100.*np.sum(list(allvals)))\n",
    "    return \"{:.1f}% ({:.0f})\".format(pct,  absolute)\n",
    "\n",
    "  for (ax, pie, title) in zip(axes, pies, titles):\n",
    "    ax.pie(pie.values(), wedgeprops=dict(width=0.75), startangle=90, labels=pie.keys(), autopct=lambda pct: autopct_func(pct, pie.values()))\n",
    "    ax.set_title(title, fontsize=18)\n",
    "\n",
    "  fig.suptitle(fig_title, fontsize=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaLSjz4TlXnn"
   },
   "source": [
    "### Label Lexicon and Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xmus2yFYhFFk"
   },
   "outputs": [],
   "source": [
    "train_entities = [entity['category'] for sample in train for entity in sample['entities']]\n",
    "test_entities = [entity['category'] for sample in test for entity in sample['entities']]\n",
    "dev_entities  = [entity['category'] for sample in dev  for entity in sample['entities']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "exHVaXD_oPyY",
    "outputId": "16714ba8-4934-4652-9aa7-6faf728d4d2a"
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "  * Train set entities:\\t{len(train_entities)};\n",
    "  * Test set entities:\\t{len(test_entities)};\n",
    "  * Dev set entities: \\t{len(dev_entities)};\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylq0asX-nvlR"
   },
   "outputs": [],
   "source": [
    "counts = list()\n",
    "for dataset_entities in [train_entities, test_entities, dev_entities]:\n",
    "  counts.append({e: dataset_entities.count(e) for e in set(dataset_entities)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Px7JjIBSgdYq",
    "outputId": "9c2ca2cd-a4b8-42ea-ca8c-54ec91ee0f6c"
   },
   "outputs": [],
   "source": [
    "piecharts(*counts, titles=['Train Set', 'Test Set', 'Dev Set'], fig_title='Entities Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGI2IBjoQul4"
   },
   "source": [
    "### Entities Spans Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMnGZEBHCnZf"
   },
   "outputs": [],
   "source": [
    "def entities_length(data):\n",
    "  token_lengths = dict()\n",
    "  chars_lengths = dict()\n",
    "  for sample in data:\n",
    "    for entity in sample['entities']:\n",
    "      e = entity['category']\n",
    "      if e not in token_lengths:\n",
    "        token_lengths[e] = list()\n",
    "        chars_lengths[e] = list()\n",
    "      chars_lengths[e].append(len(entity['title']))\n",
    "      token_lengths[e].append(len(entity['title'].split()) + 1)\n",
    "\n",
    "  assert list(chars_lengths.keys()) == list(token_lengths.keys())\n",
    "\n",
    "  keys = chars_lengths.keys()\n",
    "  \n",
    "  return pd.DataFrame(\n",
    "    index=keys, \n",
    "    data={'chars' : np.array([*chars_lengths.values()]), \n",
    "          'tokens': np.array([*token_lengths.values()])}\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a5p_o5d-SU_w",
    "outputId": "11d378f8-3670-472c-a9b7-c3f1986b0cb3"
   },
   "outputs": [],
   "source": [
    "plot_histogram(entities_length(train), title='Train Entities Length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grk8ffUJbgj-"
   },
   "source": [
    "### Nesting and Overlap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0iEWDdcIgsGA"
   },
   "outputs": [],
   "source": [
    "def assess_entities_overlap(example):\n",
    "  overlaps = 0\n",
    "  nestings = 0\n",
    "  entities = 0\n",
    "  \n",
    "  bitmap = np.zeros(len(example['text']))\n",
    "\n",
    "  for entity in sorted(example['entities'], key=lambda e: (e['start'], -e['end'])):\n",
    "    region = bitmap[entity['start']:entity['end']]\n",
    "    if not np.max(region):\n",
    "      entities += 1  # normal entity\n",
    "    else:\n",
    "      if np.min(region) == np.max(region):\n",
    "        nestings += 1  # entity contained entirely by a longer one\n",
    "      else:\n",
    "        overlaps += 1  # entity partially overlapping with another\n",
    "    \n",
    "    bitmap[entity['start']:entity['end']] += 1.\n",
    "  \n",
    "  return entities, overlaps, nestings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQ7jHaX2vLSv"
   },
   "outputs": [],
   "source": [
    "dataset_insights = []\n",
    "\n",
    "for dataset in [train, test, dev]:\n",
    "  \n",
    "  entities_insights = {'nestings': 0, 'overlaps': 0, 'flat': 0}\n",
    "\n",
    "  for example in dataset:\n",
    "    flat, overlaps, nestings = assess_entities_overlap(example)\n",
    "    entities_insights['nestings'] += nestings\n",
    "    entities_insights['overlaps'] += overlaps\n",
    "    entities_insights['flat'] += flat\n",
    "  \n",
    "  dataset_insights.append(deepcopy(entities_insights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "lTwqec7DtzHX",
    "outputId": "6ed005ef-e9df-4aed-ceb0-5abb22205dd1"
   },
   "outputs": [],
   "source": [
    "piecharts(*dataset_insights, titles=['Train Set', 'Test Set', 'Dev Set'], fig_title='Nested Entities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9g_XTmOo7AA"
   },
   "source": [
    "I consider an entity a **nested mention** when it is **entirely contained in the span of a longer entity**. When two spans are only partially overlapping, then I consider the two entities as **overlapping**. As you can see from the chart above, the dataset only contains nested mentions, and no overlapping mentions, however, the Pyramid model is capable of extracting overlapping as well as nested entity mentions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b02zkumvpfZK"
   },
   "source": [
    "## Experiments Setup\n",
    "\n",
    "Loading the word embeddings and the lexicon, then prepare the data loaders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C529p7A5jPW0"
   },
   "outputs": [],
   "source": [
    "lexicon, word_embeddings_weights = get_bio_word_vectors('genia/glove/glove.6B.200d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iz6-csJCjye_"
   },
   "outputs": [],
   "source": [
    "train_data = PyramidNerDataset(\n",
    "  jsonline_data_reader('data/wordnet-dataset.train.jsonl'),\n",
    "  pyramid_max_depth=8,\n",
    "  token_lexicon=lexicon,\n",
    "  custom_tokenizer=None, \n",
    "  char_vectorizer=True,\n",
    ").get_dataloader(\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    device=DEVICE, \n",
    "    bucketing=True\n",
    ")\n",
    "\n",
    "test_data = PyramidNerDataset(\n",
    "  jsonline_data_reader('data/wordnet-dataset.test.jsonl'),\n",
    "  pyramid_max_depth=8,\n",
    "  token_lexicon=lexicon,\n",
    "  custom_tokenizer=None, \n",
    "  char_vectorizer=True,\n",
    ").get_dataloader(\n",
    "    shuffle=True, \n",
    "    batch_size=16,\n",
    "    device=DEVICE, \n",
    "    bucketing=True\n",
    ")\n",
    "\n",
    "dev_data = PyramidNerDataset(\n",
    "  jsonline_data_reader('data/wordnet-dataset.dev.jsonl'),\n",
    "  pyramid_max_depth=8,\n",
    "  token_lexicon=lexicon,\n",
    "  custom_tokenizer=None, \n",
    "  char_vectorizer=True,\n",
    ").get_dataloader(\n",
    "    shuffle=True, \n",
    "    batch_size=16,\n",
    "    device=DEVICE, \n",
    "    bucketing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960EPJjtqFLm"
   },
   "source": [
    "## Experiment 1: Pyramid Basic\n",
    "\n",
    "Just the standard 15-layers pyramid (plus one extra layer for the remedy solution). We use character embeddings and PubMed word embeddings. We do not fine-tune the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5O7Hxp6fkVM1"
   },
   "outputs": [],
   "source": [
    "pyramid_ner = PyramidNer(\n",
    "  word_lexicon=lexicon,\n",
    "  entities_lexicon=train_entities,\n",
    "  word_embeddings=PretrainedWordEmbeddings(word_embeddings_weights, padding_idx=0, freeze=True),\n",
    "  language_model=None,\n",
    "  char_embeddings_dim=60,\n",
    "  encoder_hidden_size=100,\n",
    "  encoder_output_size=200,\n",
    "  decoder_hidden_size=100,\n",
    "  inverse_pyramid=False,\n",
    "  custom_tokenizer=None,\n",
    "  pyramid_max_depth=8,\n",
    "  decoder_dropout=0.4,\n",
    "  encoder_dropout=0.4,\n",
    "  device=DEVICE,\n",
    ")\n",
    "\n",
    "trainer = PyramidNerTrainer(pyramid_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data.device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pB8gqNIHl7BB"
   },
   "outputs": [],
   "source": [
    "optimizer, scheduler = get_default_sgd_optim(pyramid_ner.nnet.parameters())\n",
    "\n",
    "ner_model, report = trainer.train(\n",
    "  train_data, \n",
    "  optimizer=optimizer, \n",
    "  scheduler=scheduler, \n",
    "  restore_weights_on='loss',\n",
    "  epochs=30,\n",
    "  dev_data=dev_data, \n",
    "  patience=np.inf, \n",
    "  grad_clip=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yy7enskYaMW6"
   },
   "outputs": [],
   "source": [
    "report.plot_loss_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAqJYkPh7Yz1"
   },
   "outputs": [],
   "source": [
    "report.plot_custom_report('micro_f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a07YH0nH1-el"
   },
   "outputs": [],
   "source": [
    "print(trainer.test_model(test_data, out_dict=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AeQqiUwOqtOx"
   },
   "source": [
    "## Experiment 2: Pyramid Full\n",
    "\n",
    "Bidirectional 15-layers pyramid (plus one extra layer for the remedy solution). We use character embeddings and PubMed word embeddings. We do not fine-tune the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JpWQSbetqtO3"
   },
   "outputs": [],
   "source": [
    "pyramid_ner = PyramidNer(\n",
    "  word_lexicon=lexicon,\n",
    "  entities_lexicon=train_entities,\n",
    "  word_embeddings=PretrainedWordEmbeddings(word_embeddings_weights, padding_idx=0, freeze=True),\n",
    "  language_model=None,\n",
    "  char_embeddings_dim=60,\n",
    "  encoder_hidden_size=100,\n",
    "  encoder_output_size=200,\n",
    "  decoder_hidden_size=100,\n",
    "  inverse_pyramid=True,\n",
    "  custom_tokenizer=None,\n",
    "  pyramid_max_depth=8,\n",
    "  decoder_dropout=0.4,\n",
    "  encoder_dropout=0.4,\n",
    "  device=DEVICE,\n",
    ")\n",
    "\n",
    "trainer = PyramidNerTrainer(pyramid_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lP7pM82JqtO9"
   },
   "outputs": [],
   "source": [
    "optimizer, scheduler = get_default_sgd_optim(pyramid_ner.nnet.parameters())\n",
    "ner_model, report = trainer.train(\n",
    "  train_data, \n",
    "  optimizer=optimizer, \n",
    "  scheduler=scheduler, \n",
    "  restore_weights_on='loss',\n",
    "  epochs=30,\n",
    "  dev_data=dev_data, \n",
    "  patience=np.inf, \n",
    "  grad_clip=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jujiA_1KqtO_"
   },
   "outputs": [],
   "source": [
    "report.plot_loss_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AaflTBeTqtPC"
   },
   "outputs": [],
   "source": [
    "report.plot_custom_report('micro_f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTvNEEusqtPF"
   },
   "outputs": [],
   "source": [
    "print(trainer.test_model(test_data, out_dict=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V73gMJt7ra0M"
   },
   "source": [
    "## Experiment 3: Pyramid Full + BioBERT\n",
    "\n",
    "Bidirectional 15-layers pyramid (plus one extra layer for the remedy solution). We use character embeddings, PubMed word embeddings and BioBERT word embeddings obtained from the token embeddings using [Flair](https://github.com/zalandoresearch/flair). We do not fine-tune neither of the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lU4hTg8Wra0S"
   },
   "outputs": [],
   "source": [
    "pyramid_ner = PyramidNer(\n",
    "  word_lexicon=lexicon,\n",
    "  entities_lexicon=train_entities,\n",
    "  word_embeddings=PretrainedWordEmbeddings(word_embeddings_weights, padding_idx=0, freeze=True),\n",
    "  language_model='bert-base-multilingual-cased',\n",
    "  char_embeddings_dim=60,\n",
    "  encoder_hidden_size=100,\n",
    "  encoder_output_size=200,\n",
    "  decoder_hidden_size=100,\n",
    "  inverse_pyramid=True,\n",
    "  custom_tokenizer=None,\n",
    "  pyramid_max_depth=8,\n",
    "  decoder_dropout=0.4,\n",
    "  encoder_dropout=0.4,\n",
    "  device=DEVICE,\n",
    ")\n",
    "\n",
    "trainer = PyramidNerTrainer(pyramid_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zUSqgS3ra0Y"
   },
   "outputs": [],
   "source": [
    "optimizer, scheduler = get_default_sgd_optim(pyramid_ner.nnet.parameters())\n",
    "ner_model, report = trainer.train(\n",
    "  train_data, \n",
    "  optimizer=optimizer, \n",
    "  scheduler=scheduler, \n",
    "  restore_weights_on='loss',\n",
    "  epochs=30,\n",
    "  dev_data=dev_data, \n",
    "  patience=np.inf, \n",
    "  grad_clip=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1o9HPseura0a"
   },
   "outputs": [],
   "source": [
    "report.plot_loss_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilf3GL9ura0d"
   },
   "outputs": [],
   "source": [
    "report.plot_custom_report('micro_f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MyhNxq54ra0f"
   },
   "outputs": [],
   "source": [
    "print(trainer.test_model(test_data, out_dict=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RC7UZtsfsCqA"
   },
   "source": [
    "## Experiment 4: Shallow Pyramid Basic\n",
    "\n",
    "This is an extra-experiment not performed in the paper to test out the **new implementation of the remedy solution**. In the paper, the remedy solution allows the model to capture entity mentions longer than the depth of the pyramid, but only if these mentions are not nested. **By re-formulating the problem in a multi-label setting, I've extended the remedy solution to also capture nested mentions that are longer than the depth of the Pyramid.**\n",
    "\n",
    "As highlighted in the paper, reducing the depth of the pyramid obviously reduces the discriminative power of the model. However, it also reduces the number of parameters and makes **inferece feasible on CPU**.\n",
    "\n",
    "Basic 2-layers pyramid (plus one extra layer for the remedy solution). We use character embeddings and PubMed word embeddings. We do not fine-tune the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Viy96IppDh3w"
   },
   "outputs": [],
   "source": [
    "train_data = PyramidNerDataset(\n",
    "  jsonline_data_reader('data/wordnet-dataset.train.jsonl'),\n",
    "  pyramid_max_depth=2,\n",
    "  token_lexicon=lexicon,\n",
    "  custom_tokenizer=None, \n",
    "  char_vectorizer=True,\n",
    ").get_dataloader(\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    device=DEVICE, \n",
    "    bucketing=True\n",
    ")\n",
    "\n",
    "test_data = PyramidNerDataset(\n",
    "  jsonline_data_reader('data/wordnet-dataset.test.jsonl'),\n",
    "  pyramid_max_depth=2,\n",
    "  token_lexicon=lexicon,\n",
    "  custom_tokenizer=None, \n",
    "  char_vectorizer=True,\n",
    ").get_dataloader(\n",
    "    shuffle=True, \n",
    "    batch_size=16,\n",
    "    device=DEVICE, \n",
    "    bucketing=True\n",
    ")\n",
    "\n",
    "dev_data = PyramidNerDataset(\n",
    "  jsonline_data_reader('data/wordnet-dataset.dev.jsonl'),\n",
    "  pyramid_max_depth=2,\n",
    "  token_lexicon=lexicon,\n",
    "  custom_tokenizer=None, \n",
    "  char_vectorizer=True,\n",
    ").get_dataloader(\n",
    "    shuffle=True, \n",
    "    batch_size=16,\n",
    "    device=DEVICE, \n",
    "    bucketing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "629d65ZWsCqF"
   },
   "outputs": [],
   "source": [
    "pyramid_ner = PyramidNer(\n",
    "  word_lexicon=lexicon,\n",
    "  entities_lexicon=train_entities,\n",
    "  word_embeddings=PretrainedWordEmbeddings(word_embeddings_weights, padding_idx=0, freeze=True),\n",
    "  language_model=None,\n",
    "  char_embeddings_dim=60,\n",
    "  encoder_hidden_size=100,\n",
    "  encoder_output_size=200,\n",
    "  decoder_hidden_size=100,\n",
    "  inverse_pyramid=True,\n",
    "  custom_tokenizer=None,\n",
    "  pyramid_max_depth=2,\n",
    "  decoder_dropout=0.2,\n",
    "  encoder_dropout=0.2,\n",
    "  device=DEVICE,\n",
    ")\n",
    "\n",
    "trainer = PyramidNerTrainer(pyramid_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmCbKBcMsCqK"
   },
   "outputs": [],
   "source": [
    "optimizer, scheduler = get_default_sgd_optim(pyramid_ner.nnet.parameters())\n",
    "ner_model, report = trainer.train(\n",
    "  train_data, \n",
    "  optimizer=optimizer, \n",
    "  scheduler=scheduler, \n",
    "  restore_weights_on='loss',\n",
    "  epochs=30,\n",
    "  dev_data=dev_data, \n",
    "  patience=np.inf, \n",
    "  grad_clip=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qTKZPlhsCqN"
   },
   "outputs": [],
   "source": [
    "report.plot_loss_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5GNxye0sCqP"
   },
   "outputs": [],
   "source": [
    "report.plot_custom_report('micro_f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcpDAhkcsCqR"
   },
   "outputs": [],
   "source": [
    "print(trainer.test_model(test_data, out_dict=False))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Pyramid Nested NER: Genia",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}